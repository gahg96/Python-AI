#!/usr/bin/env python3
"""
Gamium AlphaZero è®­ç»ƒè„šæœ¬

ç”¨æ³•:
    python train.py [--iterations N] [--games G] [--save-dir DIR]

ç¤ºä¾‹:
    python train.py --iterations 50 --games 10  # å¿«é€Ÿæµ‹è¯•
    python train.py --iterations 200 --games 20  # å®Œæ•´è®­ç»ƒ
"""

import argparse
import os
import sys
from datetime import datetime
from pathlib import Path

import numpy as np
import torch

# æ·»åŠ  src åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent / "src"))

from environment.lending_env import LendingEnv
from agents.alphazero_agent import AlphaZeroAgent
from agents.mcts import MCTSConfig
from agents.baseline_agents import (
    RandomAgent, RuleBasedAgent, ConservativeAgent, AggressiveAgent,
    evaluate_agent
)
from utils.visualization import plot_training_progress, print_comparison_table
from utils.logger import GamiumLogger


def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description="Gamium AlphaZero è®­ç»ƒ")
    
    parser.add_argument("--iterations", type=int, default=30,
                        help="è®­ç»ƒè¿­ä»£æ¬¡æ•° (é»˜è®¤: 30)")
    parser.add_argument("--games", type=int, default=5,
                        help="æ¯æ¬¡è¿­ä»£çš„è‡ªæˆ‘å¯¹å¼ˆå±€æ•° (é»˜è®¤: 5)")
    parser.add_argument("--train-steps", type=int, default=50,
                        help="æ¯æ¬¡è¿­ä»£çš„è®­ç»ƒæ­¥æ•° (é»˜è®¤: 50)")
    parser.add_argument("--batch-size", type=int, default=64,
                        help="æ‰¹é‡å¤§å° (é»˜è®¤: 64)")
    parser.add_argument("--lr", type=float, default=0.001,
                        help="å­¦ä¹ ç‡ (é»˜è®¤: 0.001)")
    parser.add_argument("--hidden-dim", type=int, default=256,
                        help="ç½‘ç»œéšè—å±‚ç»´åº¦ (é»˜è®¤: 256)")
    parser.add_argument("--mcts-simulations", type=int, default=20,
                        help="MCTS æ¨¡æ‹Ÿæ¬¡æ•° (é»˜è®¤: 20)")
    parser.add_argument("--seed", type=int, default=42,
                        help="éšæœºç§å­ (é»˜è®¤: 42)")
    parser.add_argument("--save-dir", type=str, default="experiments",
                        help="ä¿å­˜ç›®å½• (é»˜è®¤: experiments)")
    parser.add_argument("--eval-episodes", type=int, default=5,
                        help="è¯„ä¼°å›åˆæ•° (é»˜è®¤: 5)")
    parser.add_argument("--no-plot", action="store_true",
                        help="ä¸æ˜¾ç¤ºå›¾è¡¨")
    
    return parser.parse_args()


def main():
    args = parse_args()
    
    # è®¾ç½®éšæœºç§å­
    np.random.seed(args.seed)
    torch.manual_seed(args.seed)
    
    # åˆ›å»ºä¿å­˜ç›®å½•
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    save_dir = Path(args.save_dir) / f"run_{timestamp}"
    save_dir.mkdir(parents=True, exist_ok=True)
    
    # åˆå§‹åŒ–æ—¥å¿—
    logger = GamiumLogger("Gamium-Train", log_dir=str(save_dir / "logs"))
    
    logger.info("=" * 60)
    logger.info("ğŸ® Gamium AlphaZero è®­ç»ƒå¼€å§‹")
    logger.info("=" * 60)
    logger.info(f"è®­ç»ƒå‚æ•°:")
    logger.info(f"  - è¿­ä»£æ¬¡æ•°: {args.iterations}")
    logger.info(f"  - æ¯æ¬¡è¿­ä»£å¯¹å¼ˆå±€æ•°: {args.games}")
    logger.info(f"  - æ‰¹é‡å¤§å°: {args.batch_size}")
    logger.info(f"  - å­¦ä¹ ç‡: {args.lr}")
    logger.info(f"  - ä¿å­˜ç›®å½•: {save_dir}")
    
    # åˆ›å»ºç¯å¢ƒ
    env = LendingEnv(seed=args.seed)
    logger.info(f"ç¯å¢ƒå·²åˆ›å»º: çŠ¶æ€ç»´åº¦={env.observation_space.shape}, åŠ¨ä½œç»´åº¦={env.action_space.shape}")
    
    # åˆ›å»º MCTS é…ç½®
    mcts_config = MCTSConfig(
        num_simulations=args.mcts_simulations,
        c_puct=1.5,
        temperature=1.0,
    )
    
    # åˆ›å»º AlphaZero æ™ºèƒ½ä½“
    agent = AlphaZeroAgent(
        state_dim=22,
        hidden_dim=args.hidden_dim,
        lr=args.lr,
        mcts_config=mcts_config,
        use_simple_mcts=True  # POC ä½¿ç”¨ç®€åŒ–ç‰ˆ
    )
    
    logger.info(f"AlphaZero æ™ºèƒ½ä½“å·²åˆ›å»º")
    logger.info(f"  - ç½‘ç»œå‚æ•°é‡: {sum(p.numel() for p in agent.network.parameters()):,}")
    logger.info(f"  - åŠ¨ä½œç©ºé—´å¤§å°: {agent.network.NUM_ACTIONS}")
    
    # è®­ç»ƒ
    logger.info("\n" + "=" * 60)
    logger.info("ğŸš€ å¼€å§‹è®­ç»ƒ...")
    logger.info("=" * 60 + "\n")
    
    train_stats = agent.train(
        env=env,
        num_iterations=args.iterations,
        games_per_iteration=args.games,
        train_steps_per_iteration=args.train_steps,
        batch_size=args.batch_size,
        verbose=True
    )
    
    # ä¿å­˜æ¨¡å‹
    model_path = save_dir / "alphazero_model.pt"
    agent.save(str(model_path))
    
    # è®°å½•è®­ç»ƒæŒ‡æ ‡
    for stat in train_stats['iterations']:
        logger.log_metric("reward", stat['avg_reward'], step=stat['iteration'])
        logger.log_metric("profit", stat['avg_profit'], step=stat['iteration'])
        logger.log_metric("bankruptcy_rate", stat['bankruptcy_rate'], step=stat['iteration'])
        logger.log_metric("loss", stat['avg_loss'], step=stat['iteration'])
    
    logger.save_metrics()
    
    # è¯„ä¼°å¹¶ä¸åŸºçº¿å¯¹æ¯”
    logger.info("\n" + "=" * 60)
    logger.info("ğŸ“Š è¯„ä¼° AlphaZero vs åŸºçº¿ç­–ç•¥")
    logger.info("=" * 60 + "\n")
    
    # åˆ›å»ºåŸºçº¿æ™ºèƒ½ä½“
    baseline_agents = [
        RandomAgent(seed=args.seed),
        RuleBasedAgent(),
        ConservativeAgent(),
        AggressiveAgent(),
    ]
    
    results = []
    
    # è¯„ä¼°åŸºçº¿
    for baseline in baseline_agents:
        result = evaluate_agent(baseline, env, num_episodes=args.eval_episodes)
        results.append(result)
        logger.info(f"{result['agent_name']}: å¥–åŠ±={result['avg_reward']:.2f}, "
                    f"åˆ©æ¶¦={result['avg_profit']:.1f}äº¿, NPL={result['avg_npl']:.2%}")
    
    # è¯„ä¼° AlphaZero
    class AlphaZeroWrapper:
        def __init__(self, agent, env):
            self.name = "AlphaZero"
            self.agent = agent
            self.env = env
        
        def select_action(self, state, info=None):
            return self.agent.select_action(state, env=self.env, deterministic=True)
    
    az_wrapper = AlphaZeroWrapper(agent, env)
    az_result = evaluate_agent(az_wrapper, env, num_episodes=args.eval_episodes)
    az_result['agent_name'] = "AlphaZero"
    results.append(az_result)
    
    logger.info(f"AlphaZero: å¥–åŠ±={az_result['avg_reward']:.2f}, "
                f"åˆ©æ¶¦={az_result['avg_profit']:.1f}äº¿, NPL={az_result['avg_npl']:.2%}")
    
    # æ‰“å°å¯¹æ¯”è¡¨æ ¼
    print_comparison_table(results)
    
    # ç»˜åˆ¶è®­ç»ƒæ›²çº¿
    if not args.no_plot:
        plot_path = save_dir / "training_progress.png"
        plot_training_progress(
            train_stats['iterations'],
            save_path=str(plot_path),
            show=True
        )
    
    logger.info("\n" + "=" * 60)
    logger.info("âœ… è®­ç»ƒå®Œæˆ!")
    logger.info(f"   æ¨¡å‹ä¿å­˜äº: {model_path}")
    logger.info(f"   æ—¥å¿—ä¿å­˜äº: {save_dir / 'logs'}")
    logger.info("=" * 60)
    
    return train_stats


if __name__ == "__main__":
    main()

