# ç‰¹å¾æ–‡ä»¶æ ¼å¼ä¸è®­ç»ƒæŒ‡å—

## ğŸ“‹ ç›®å½•
1. [ç‰¹å¾æ–‡ä»¶æ ¼å¼](#ç‰¹å¾æ–‡ä»¶æ ¼å¼)
2. [ç‰¹å¾æ–‡ä»¶æ ·ä¾‹](#ç‰¹å¾æ–‡ä»¶æ ·ä¾‹)
3. [æ¨¡å‹è®­ç»ƒæµç¨‹](#æ¨¡å‹è®­ç»ƒæµç¨‹)
4. [è®­ç»ƒä»£ç ç¤ºä¾‹](#è®­ç»ƒä»£ç ç¤ºä¾‹)
5. [æ¨¡å‹è¯„ä¼°ä¸éƒ¨ç½²](#æ¨¡å‹è¯„ä¼°ä¸éƒ¨ç½²)

---

## ğŸ“Š ç‰¹å¾æ–‡ä»¶æ ¼å¼

### 1. æ–‡ä»¶æ ¼å¼

**æ¨èæ ¼å¼ï¼šParquet**
- âœ… åˆ—å¼å­˜å‚¨ï¼Œè¯»å–é€Ÿåº¦å¿«
- âœ… å‹ç¼©ç‡é«˜ï¼ŒèŠ‚çœå­˜å‚¨ç©ºé—´
- âœ… æ”¯æŒæ•°æ®ç±»å‹ä¿ç•™
- âœ… æ”¯æŒåˆ†åŒºå­˜å‚¨

**å¤‡é€‰æ ¼å¼ï¼šCSV**
- âœ… é€šç”¨æ€§å¼ºï¼Œæ˜“äºæŸ¥çœ‹
- âŒ æ–‡ä»¶è¾ƒå¤§ï¼Œè¯»å–è¾ƒæ…¢
- âŒ æ•°æ®ç±»å‹å¯èƒ½ä¸¢å¤±

### 2. ç‰¹å¾æ–‡ä»¶ç»“æ„

ç‰¹å¾æ–‡ä»¶åº”åŒ…å«ä»¥ä¸‹éƒ¨åˆ†ï¼š

```
ç‰¹å¾æ–‡ä»¶ (customer_features.parquet)
â”œâ”€â”€ å®¢æˆ·æ ‡è¯†
â”‚   â””â”€â”€ customer_id (å“ˆå¸ŒIDï¼Œå·²è„±æ•)
â”œâ”€â”€ åŸºç¡€ç‰¹å¾ (ä»CRMç³»ç»Ÿ)
â”‚   â”œâ”€â”€ age (å¹´é¾„)
â”‚   â”œâ”€â”€ gender (æ€§åˆ«)
â”‚   â”œâ”€â”€ education (æ•™è‚²æ°´å¹³)
â”‚   â”œâ”€â”€ industry (è¡Œä¸š)
â”‚   â”œâ”€â”€ city_tier (åŸå¸‚ç­‰çº§)
â”‚   â””â”€â”€ customer_type (å®¢æˆ·ç±»å‹)
â”œâ”€â”€ è´¢åŠ¡ç‰¹å¾ (ä»æ ¸å¿ƒç³»ç»Ÿ)
â”‚   â”œâ”€â”€ monthly_income (æœˆæ”¶å…¥)
â”‚   â”œâ”€â”€ total_assets (æ€»èµ„äº§)
â”‚   â”œâ”€â”€ total_liabilities (æ€»è´Ÿå€º)
â”‚   â”œâ”€â”€ debt_ratio (è´Ÿå€ºç‡)
â”‚   â”œâ”€â”€ debt_to_income (å€ºåŠ¡æ”¶å…¥æ¯”)
â”‚   â”œâ”€â”€ total_deposit_balance (å­˜æ¬¾ä½™é¢)
â”‚   â””â”€â”€ avg_account_balance (å¹³å‡è´¦æˆ·ä½™é¢)
â”œâ”€â”€ äº¤æ˜“ç‰¹å¾ (ä»æ ¸å¿ƒç³»ç»Ÿ)
â”‚   â”œâ”€â”€ total_income (æ€»æ”¶å…¥)
â”‚   â”œâ”€â”€ total_expense (æ€»æ”¯å‡º)
â”‚   â”œâ”€â”€ savings_rate (å‚¨è“„ç‡)
â”‚   â”œâ”€â”€ income_volatility (æ”¶å…¥æ³¢åŠ¨ç‡)
â”‚   â”œâ”€â”€ transaction_count (äº¤æ˜“æ¬¡æ•°)
â”‚   â””â”€â”€ avg_transaction_amount (å¹³å‡äº¤æ˜“é‡‘é¢)
â”œâ”€â”€ è´·æ¬¾å†å²ç‰¹å¾ (ä»ä¿¡è´·ç³»ç»Ÿ)
â”‚   â”œâ”€â”€ total_loans (å†å²è´·æ¬¾æ¬¡æ•°)
â”‚   â”œâ”€â”€ total_loan_amount (å†å²è´·æ¬¾æ€»é¢)
â”‚   â”œâ”€â”€ avg_loan_amount (å¹³å‡è´·æ¬¾é‡‘é¢)
â”‚   â”œâ”€â”€ default_count (è¿çº¦æ¬¡æ•°)
â”‚   â”œâ”€â”€ max_overdue_days (æœ€å¤§é€¾æœŸå¤©æ•°)
â”‚   â”œâ”€â”€ avg_interest_rate (å¹³å‡åˆ©ç‡)
â”‚   â””â”€â”€ months_since_last_loan (è·ä¸Šæ¬¡è´·æ¬¾æœˆæ•°)
â”œâ”€â”€ æ—¶é—´ç‰¹å¾
â”‚   â”œâ”€â”€ months_as_customer (æˆä¸ºå®¢æˆ·æœˆæ•°)
â”‚   â””â”€â”€ registration_date (æ³¨å†Œæ—¥æœŸ)
â””â”€â”€ æ ‡ç­¾ (ç›®æ ‡å˜é‡)
    â”œâ”€â”€ defaulted (æ˜¯å¦è¿çº¦: 0/1)
    â”œâ”€â”€ default_probability (è¿çº¦æ¦‚ç‡: 0-1)
    â””â”€â”€ loan_status (è´·æ¬¾çŠ¶æ€: normal/overdue/defaulted)
```

---

## ğŸ“ ç‰¹å¾æ–‡ä»¶æ ·ä¾‹

### æ ·ä¾‹1ï¼šå®Œæ•´ç‰¹å¾æ–‡ä»¶ï¼ˆParquetæ ¼å¼ï¼‰

```python
import pandas as pd
import numpy as np

# åˆ›å»ºç¤ºä¾‹ç‰¹å¾æ•°æ®
sample_features = pd.DataFrame({
    # å®¢æˆ·æ ‡è¯†
    'customer_id': ['C001', 'C002', 'C003', 'C004', 'C005'],
    
    # åŸºç¡€ç‰¹å¾
    'age': [35, 42, 28, 55, 38],
    'gender': ['M', 'F', 'M', 'F', 'M'],
    'education': ['bachelor', 'master', 'bachelor', 'high_school', 'college'],
    'industry': ['it', 'finance', 'manufacturing', 'service', 'retail'],
    'city_tier': ['tier_1', 'tier_2', 'tier_1', 'tier_3', 'tier_2'],
    'customer_type': ['salaried', 'small_business', 'salaried', 'farmer', 'freelancer'],
    
    # è´¢åŠ¡ç‰¹å¾
    'monthly_income': [15000.0, 25000.0, 12000.0, 8000.0, 18000.0],
    'total_assets': [500000.0, 1200000.0, 300000.0, 200000.0, 450000.0],
    'total_liabilities': [200000.0, 600000.0, 150000.0, 100000.0, 250000.0],
    'debt_ratio': [0.4, 0.5, 0.5, 0.5, 0.56],
    'debt_to_income': [1.11, 2.0, 1.04, 1.04, 1.16],
    'total_deposit_balance': [100000.0, 300000.0, 50000.0, 30000.0, 80000.0],
    'avg_account_balance': [50000.0, 150000.0, 25000.0, 15000.0, 40000.0],
    
    # äº¤æ˜“ç‰¹å¾
    'total_income': [180000.0, 300000.0, 144000.0, 96000.0, 216000.0],
    'total_expense': [150000.0, 250000.0, 120000.0, 80000.0, 180000.0],
    'savings_rate': [0.17, 0.17, 0.17, 0.17, 0.17],
    'income_volatility': [0.15, 0.12, 0.20, 0.25, 0.18],
    'transaction_count': [120, 180, 100, 80, 150],
    'avg_transaction_amount': [1500.0, 1667.0, 1440.0, 1200.0, 1440.0],
    
    # è´·æ¬¾å†å²ç‰¹å¾
    'total_loans': [3, 5, 2, 1, 4],
    'total_loan_amount': [300000.0, 800000.0, 200000.0, 100000.0, 500000.0],
    'avg_loan_amount': [100000.0, 160000.0, 100000.0, 100000.0, 125000.0],
    'default_count': [0, 1, 0, 0, 0],
    'max_overdue_days': [0, 45, 0, 0, 0],
    'avg_interest_rate': [0.06, 0.07, 0.06, 0.05, 0.065],
    'months_since_last_loan': [6, 3, 12, 24, 4],
    
    # æ—¶é—´ç‰¹å¾
    'months_as_customer': [36, 60, 24, 120, 48],
    'registration_date': ['2018-01-15', '2015-03-20', '2020-05-10', '2012-08-05', '2017-11-30'],
    
    # æ ‡ç­¾
    'defaulted': [0, 1, 0, 0, 0],
    'default_probability': [0.05, 0.35, 0.08, 0.12, 0.15],
    'loan_status': ['normal', 'defaulted', 'normal', 'normal', 'normal']
})

# ä¿å­˜ä¸ºParquetæ ¼å¼
sample_features.to_parquet('data/sample_features.parquet', index=False)
print("âœ… ç¤ºä¾‹ç‰¹å¾æ–‡ä»¶å·²ä¿å­˜")
```

### æ ·ä¾‹2ï¼šCSVæ ¼å¼æŸ¥çœ‹

```python
# è¯»å–å¹¶æŸ¥çœ‹ç‰¹å¾æ–‡ä»¶
features = pd.read_parquet('data/sample_features.parquet')
print(features.head())
print(f"\nç‰¹å¾ç»´åº¦: {features.shape}")
print(f"ç‰¹å¾åˆ—è¡¨:\n{features.columns.tolist()}")
```

**è¾“å‡ºç¤ºä¾‹ï¼š**
```
   customer_id  age gender education    industry city_tier customer_type  monthly_income  ...
0         C001   35      M  bachelor          it    tier_1     salaried        15000.0  ...
1         C002   42      F   master     finance    tier_2 small_business        25000.0  ...
2         C003   28      M  bachelor manufacturing    tier_1     salaried        12000.0  ...
3         C004   55      F high_school     service    tier_3       farmer         8000.0  ...
4         C005   38      M    college      retail    tier_2   freelancer        18000.0  ...

ç‰¹å¾ç»´åº¦: (5, 35)
ç‰¹å¾åˆ—è¡¨: ['customer_id', 'age', 'gender', 'education', 'industry', ...]
```

### æ ·ä¾‹3ï¼šç‰¹å¾ç»Ÿè®¡ä¿¡æ¯

```python
# æŸ¥çœ‹ç‰¹å¾ç»Ÿè®¡ä¿¡æ¯
print("æ•°å€¼ç‰¹å¾ç»Ÿè®¡:")
print(features.select_dtypes(include=[np.number]).describe())

print("\nåˆ†ç±»ç‰¹å¾ç»Ÿè®¡:")
print(features.select_dtypes(include=['object']).describe())
```

---

## ğŸ¯ æ¨¡å‹è®­ç»ƒæµç¨‹

### 1. è®­ç»ƒæµç¨‹æ¦‚è§ˆ

```
ç‰¹å¾æ–‡ä»¶
    â†“
æ•°æ®åŠ è½½ä¸é¢„å¤„ç†
    â†“
ç‰¹å¾é€‰æ‹©ä¸å·¥ç¨‹
    â†“
æ•°æ®åˆ†å‰² (è®­ç»ƒé›†/éªŒè¯é›†/æµ‹è¯•é›†)
    â†“
æ¨¡å‹è®­ç»ƒ
    â†“
æ¨¡å‹è¯„ä¼°
    â†“
æ¨¡å‹ä¿å­˜
    â†“
æ¨¡å‹éƒ¨ç½²
```

### 2. è¯¦ç»†æ­¥éª¤

#### æ­¥éª¤1ï¼šæ•°æ®åŠ è½½

```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split

# åŠ è½½ç‰¹å¾æ–‡ä»¶
features = pd.read_parquet('data/extracted/customer_features.parquet')
print(f"æ•°æ®é‡: {len(features):,} æ¡")
print(f"ç‰¹å¾æ•°: {len(features.columns)} ä¸ª")
```

#### æ­¥éª¤2ï¼šæ•°æ®é¢„å¤„ç†

```python
# å¤„ç†ç¼ºå¤±å€¼
features = features.fillna(features.median(numeric_only=True))

# å¤„ç†åˆ†ç±»ç‰¹å¾ï¼ˆç¼–ç ï¼‰
from sklearn.preprocessing import LabelEncoder

categorical_cols = ['gender', 'education', 'industry', 'city_tier', 'customer_type']
label_encoders = {}

for col in categorical_cols:
    if col in features.columns:
        le = LabelEncoder()
        features[col] = le.fit_transform(features[col].astype(str))
        label_encoders[col] = le

# ç‰¹å¾æ ‡å‡†åŒ–
from sklearn.preprocessing import StandardScaler

numeric_cols = features.select_dtypes(include=[np.number]).columns
numeric_cols = [col for col in numeric_cols if col not in ['defaulted', 'default_probability']]

scaler = StandardScaler()
features[numeric_cols] = scaler.fit_transform(features[numeric_cols])
```

#### æ­¥éª¤3ï¼šç‰¹å¾é€‰æ‹©

```python
# é€‰æ‹©ç‰¹å¾å’Œç›®æ ‡å˜é‡
feature_cols = [
    'age', 'gender', 'education', 'industry', 'city_tier',
    'monthly_income', 'total_assets', 'total_liabilities',
    'debt_ratio', 'debt_to_income', 'total_deposit_balance',
    'savings_rate', 'income_volatility',
    'total_loans', 'default_count', 'max_overdue_days',
    'months_as_customer', 'months_since_last_loan'
]

X = features[feature_cols]
y = features['defaulted']  # ç›®æ ‡å˜é‡ï¼šæ˜¯å¦è¿çº¦

print(f"ç‰¹å¾çŸ©é˜µå½¢çŠ¶: {X.shape}")
print(f"ç›®æ ‡å˜é‡åˆ†å¸ƒ:\n{y.value_counts()}")
```

#### æ­¥éª¤4ï¼šæ•°æ®åˆ†å‰²

```python
# åˆ†å‰²æ•°æ®é›†
X_train, X_test, y_train, y_test = train_test_split(
    X, y, 
    test_size=0.2, 
    random_state=42, 
    stratify=y  # ä¿æŒç±»åˆ«åˆ†å¸ƒ
)

# è¿›ä¸€æ­¥åˆ†å‰²è®­ç»ƒé›†å’ŒéªŒè¯é›†
X_train, X_val, y_train, y_val = train_test_split(
    X_train, y_train,
    test_size=0.2,
    random_state=42,
    stratify=y_train
)

print(f"è®­ç»ƒé›†: {X_train.shape[0]:,} æ¡")
print(f"éªŒè¯é›†: {X_val.shape[0]:,} æ¡")
print(f"æµ‹è¯•é›†: {X_test.shape[0]:,} æ¡")
```

#### æ­¥éª¤5ï¼šæ¨¡å‹è®­ç»ƒ

```python
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix

# è®­ç»ƒå¤šä¸ªæ¨¡å‹
models = {
    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),
    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),
    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42)
}

results = {}

for name, model in models.items():
    print(f"\nè®­ç»ƒ {name}...")
    
    # è®­ç»ƒ
    model.fit(X_train, y_train)
    
    # é¢„æµ‹
    y_pred = model.predict(X_test)
    y_pred_proba = model.predict_proba(X_test)[:, 1]
    
    # è¯„ä¼°
    auc = roc_auc_score(y_test, y_pred_proba)
    report = classification_report(y_test, y_pred)
    
    results[name] = {
        'model': model,
        'auc': auc,
        'report': report
    }
    
    print(f"AUC Score: {auc:.4f}")
    print(f"\nåˆ†ç±»æŠ¥å‘Š:\n{report}")

# é€‰æ‹©æœ€ä½³æ¨¡å‹
best_model_name = max(results, key=lambda x: results[x]['auc'])
best_model = results[best_model_name]['model']
print(f"\næœ€ä½³æ¨¡å‹: {best_model_name} (AUC: {results[best_model_name]['auc']:.4f})")
```

---

## ğŸ’» è®­ç»ƒä»£ç ç¤ºä¾‹

### å®Œæ•´è®­ç»ƒè„šæœ¬

åˆ›å»º `scripts/train_model.py`ï¼š

```python
#!/usr/bin/env python3
"""
æ¨¡å‹è®­ç»ƒè„šæœ¬

ä½¿ç”¨æ–¹æ³•:
    python3 train_model.py --features data/extracted/customer_features.parquet --output models/
"""

import argparse
import pandas as pd
import numpy as np
import pickle
import json
from pathlib import Path
from datetime import datetime

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.metrics import (
    classification_report, 
    roc_auc_score, 
    confusion_matrix,
    precision_recall_curve
)

class ModelTrainer:
    """æ¨¡å‹è®­ç»ƒå™¨"""
    
    def __init__(self, features_path: str, output_dir: str):
        self.features_path = Path(features_path)
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        self.scaler = StandardScaler()
        self.label_encoders = {}
        self.model = None
        self.feature_cols = None
        
    def load_data(self):
        """åŠ è½½ç‰¹å¾æ•°æ®"""
        print(f"ğŸ“Š åŠ è½½ç‰¹å¾æ–‡ä»¶: {self.features_path}")
        self.features = pd.read_parquet(self.features_path)
        print(f"   âœ… åŠ è½½å®Œæˆ: {len(self.features):,} æ¡è®°å½•, {len(self.features.columns)} ä¸ªç‰¹å¾")
        return self.features
    
    def preprocess_data(self):
        """æ•°æ®é¢„å¤„ç†"""
        print("\nğŸ”§ æ•°æ®é¢„å¤„ç†...")
        
        # å¤„ç†ç¼ºå¤±å€¼
        numeric_cols = self.features.select_dtypes(include=[np.number]).columns
        self.features[numeric_cols] = self.features[numeric_cols].fillna(
            self.features[numeric_cols].median()
        )
        
        # å¤„ç†åˆ†ç±»ç‰¹å¾
        categorical_cols = ['gender', 'education', 'industry', 'city_tier', 'customer_type']
        for col in categorical_cols:
            if col in self.features.columns:
                le = LabelEncoder()
                self.features[col] = le.fit_transform(self.features[col].astype(str))
                self.label_encoders[col] = le
        
        print("   âœ… é¢„å¤„ç†å®Œæˆ")
    
    def prepare_features(self):
        """å‡†å¤‡ç‰¹å¾å’Œç›®æ ‡å˜é‡"""
        print("\nğŸ¯ å‡†å¤‡ç‰¹å¾å’Œç›®æ ‡å˜é‡...")
        
        # é€‰æ‹©ç‰¹å¾åˆ—
        self.feature_cols = [
            'age', 'gender', 'education', 'industry', 'city_tier',
            'monthly_income', 'total_assets', 'total_liabilities',
            'debt_ratio', 'debt_to_income', 'total_deposit_balance',
            'savings_rate', 'income_volatility',
            'total_loans', 'default_count', 'max_overdue_days',
            'months_as_customer', 'months_since_last_loan'
        ]
        
        # åªé€‰æ‹©å­˜åœ¨çš„åˆ—
        self.feature_cols = [col for col in self.feature_cols if col in self.features.columns]
        
        X = self.features[self.feature_cols]
        y = self.features['defaulted']
        
        print(f"   ç‰¹å¾æ•°: {len(self.feature_cols)}")
        print(f"   ç›®æ ‡å˜é‡åˆ†å¸ƒ:\n{y.value_counts()}")
        
        return X, y
    
    def split_data(self, X, y):
        """åˆ†å‰²æ•°æ®é›†"""
        print("\nğŸ“Š åˆ†å‰²æ•°æ®é›†...")
        
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=y
        )
        
        X_train, X_val, y_train, y_val = train_test_split(
            X_train, y_train, test_size=0.2, random_state=42, stratify=y_train
        )
        
        print(f"   è®­ç»ƒé›†: {X_train.shape[0]:,} æ¡")
        print(f"   éªŒè¯é›†: {X_val.shape[0]:,} æ¡")
        print(f"   æµ‹è¯•é›†: {X_test.shape[0]:,} æ¡")
        
        return X_train, X_val, X_test, y_train, y_val, y_test
    
    def scale_features(self, X_train, X_val, X_test):
        """ç‰¹å¾æ ‡å‡†åŒ–"""
        print("\nğŸ“ ç‰¹å¾æ ‡å‡†åŒ–...")
        
        X_train_scaled = self.scaler.fit_transform(X_train)
        X_val_scaled = self.scaler.transform(X_val)
        X_test_scaled = self.scaler.transform(X_test)
        
        print("   âœ… æ ‡å‡†åŒ–å®Œæˆ")
        
        return X_train_scaled, X_val_scaled, X_test_scaled
    
    def train_model(self, X_train, y_train, X_val, y_val):
        """è®­ç»ƒæ¨¡å‹"""
        print("\nğŸš€ è®­ç»ƒæ¨¡å‹...")
        
        self.model = GradientBoostingClassifier(
            n_estimators=100,
            learning_rate=0.1,
            max_depth=5,
            random_state=42,
            verbose=1
        )
        
        self.model.fit(X_train, y_train)
        
        # éªŒè¯é›†è¯„ä¼°
        val_pred_proba = self.model.predict_proba(X_val)[:, 1]
        val_auc = roc_auc_score(y_val, val_pred_proba)
        
        print(f"   âœ… è®­ç»ƒå®Œæˆ")
        print(f"   éªŒè¯é›† AUC: {val_auc:.4f}")
        
        return self.model
    
    def evaluate_model(self, X_test, y_test):
        """è¯„ä¼°æ¨¡å‹"""
        print("\nğŸ“ˆ æ¨¡å‹è¯„ä¼°...")
        
        y_pred = self.model.predict(X_test)
        y_pred_proba = self.model.predict_proba(X_test)[:, 1]
        
        # è®¡ç®—æŒ‡æ ‡
        auc = roc_auc_score(y_test, y_pred_proba)
        report = classification_report(y_test, y_pred)
        cm = confusion_matrix(y_test, y_pred)
        
        print(f"AUC Score: {auc:.4f}")
        print(f"\nåˆ†ç±»æŠ¥å‘Š:\n{report}")
        print(f"\næ··æ·†çŸ©é˜µ:\n{cm}")
        
        return {
            'auc': auc,
            'report': report,
            'confusion_matrix': cm.tolist()
        }
    
    def save_model(self, metrics):
        """ä¿å­˜æ¨¡å‹"""
        print("\nğŸ’¾ ä¿å­˜æ¨¡å‹...")
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # ä¿å­˜æ¨¡å‹
        model_path = self.output_dir / f'model_{timestamp}.pkl'
        with open(model_path, 'wb') as f:
            pickle.dump(self.model, f)
        
        # ä¿å­˜scaler
        scaler_path = self.output_dir / f'scaler_{timestamp}.pkl'
        with open(scaler_path, 'wb') as f:
            pickle.dump(self.scaler, f)
        
        # ä¿å­˜label encoders
        encoders_path = self.output_dir / f'encoders_{timestamp}.pkl'
        with open(encoders_path, 'wb') as f:
            pickle.dump(self.label_encoders, f)
        
        # ä¿å­˜ç‰¹å¾åˆ—è¡¨
        feature_info = {
            'feature_cols': self.feature_cols,
            'metrics': metrics,
            'timestamp': timestamp,
            'model_path': str(model_path),
            'scaler_path': str(scaler_path),
            'encoders_path': str(encoders_path)
        }
        
        info_path = self.output_dir / f'model_info_{timestamp}.json'
        with open(info_path, 'w') as f:
            json.dump(feature_info, f, indent=2, ensure_ascii=False)
        
        print(f"   âœ… æ¨¡å‹å·²ä¿å­˜:")
        print(f"     æ¨¡å‹: {model_path}")
        print(f"     Scaler: {scaler_path}")
        print(f"     Encoders: {encoders_path}")
        print(f"     ä¿¡æ¯: {info_path}")
        
        return model_path
    
    def run(self):
        """è¿è¡Œå®Œæ•´è®­ç»ƒæµç¨‹"""
        print("=" * 60)
        print("ğŸ¯ æ¨¡å‹è®­ç»ƒæµç¨‹")
        print("=" * 60)
        
        # 1. åŠ è½½æ•°æ®
        self.load_data()
        
        # 2. é¢„å¤„ç†
        self.preprocess_data()
        
        # 3. å‡†å¤‡ç‰¹å¾
        X, y = self.prepare_features()
        
        # 4. åˆ†å‰²æ•°æ®
        X_train, X_val, X_test, y_train, y_val, y_test = self.split_data(X, y)
        
        # 5. æ ‡å‡†åŒ–
        X_train_scaled, X_val_scaled, X_test_scaled = self.scale_features(
            X_train, X_val, X_test
        )
        
        # 6. è®­ç»ƒæ¨¡å‹
        self.train_model(X_train_scaled, y_train, X_val_scaled, y_val)
        
        # 7. è¯„ä¼°æ¨¡å‹
        metrics = self.evaluate_model(X_test_scaled, y_test)
        
        # 8. ä¿å­˜æ¨¡å‹
        model_path = self.save_model(metrics)
        
        print("\n" + "=" * 60)
        print("âœ… è®­ç»ƒå®Œæˆï¼")
        print("=" * 60)
        
        return model_path

if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='æ¨¡å‹è®­ç»ƒè„šæœ¬')
    parser.add_argument('--features', type=str, required=True,
                       help='ç‰¹å¾æ–‡ä»¶è·¯å¾„ (Parquetæ ¼å¼)')
    parser.add_argument('--output', type=str, default='models',
                       help='æ¨¡å‹è¾“å‡ºç›®å½•')
    
    args = parser.parse_args()
    
    trainer = ModelTrainer(args.features, args.output)
    trainer.run()
```

---

## ğŸ“Š ä½¿ç”¨ç¤ºä¾‹

### è¿è¡Œè®­ç»ƒ

```bash
# ä½¿ç”¨æå–çš„ç‰¹å¾æ–‡ä»¶è¿›è¡Œè®­ç»ƒ
python3 scripts/train_model.py \
    --features data/extracted/customer_features.parquet \
    --output models/
```

### è®­ç»ƒè¾“å‡º

```
============================================================
ğŸ¯ æ¨¡å‹è®­ç»ƒæµç¨‹
============================================================
ğŸ“Š åŠ è½½ç‰¹å¾æ–‡ä»¶: data/extracted/customer_features.parquet
   âœ… åŠ è½½å®Œæˆ: 5,000,000 æ¡è®°å½•, 35 ä¸ªç‰¹å¾

ğŸ”§ æ•°æ®é¢„å¤„ç†...
   âœ… é¢„å¤„ç†å®Œæˆ

ğŸ¯ å‡†å¤‡ç‰¹å¾å’Œç›®æ ‡å˜é‡...
   ç‰¹å¾æ•°: 18
   ç›®æ ‡å˜é‡åˆ†å¸ƒ:
   0    4750000
   1     250000

ğŸ“Š åˆ†å‰²æ•°æ®é›†...
   è®­ç»ƒé›†: 3,200,000 æ¡
   éªŒè¯é›†: 800,000 æ¡
   æµ‹è¯•é›†: 1,000,000 æ¡

ğŸ“ ç‰¹å¾æ ‡å‡†åŒ–...
   âœ… æ ‡å‡†åŒ–å®Œæˆ

ğŸš€ è®­ç»ƒæ¨¡å‹...
   âœ… è®­ç»ƒå®Œæˆ
   éªŒè¯é›† AUC: 0.8523

ğŸ“ˆ æ¨¡å‹è¯„ä¼°...
AUC Score: 0.8541

åˆ†ç±»æŠ¥å‘Š:
              precision    recall  f1-score   support
           0       0.98      0.99      0.99    950000
           1       0.85      0.72      0.78     50000

ğŸ’¾ ä¿å­˜æ¨¡å‹...
   âœ… æ¨¡å‹å·²ä¿å­˜:
     æ¨¡å‹: models/model_20241209_143022.pkl
     Scaler: models/scaler_20241209_143022.pkl
     Encoders: models/encoders_20241209_143022.pkl
     ä¿¡æ¯: models/model_info_20241209_143022.json

============================================================
âœ… è®­ç»ƒå®Œæˆï¼
============================================================
```

---

## ğŸ” æ¨¡å‹è¯„ä¼°ä¸éƒ¨ç½²

### æ¨¡å‹è¯„ä¼°æŒ‡æ ‡

- **AUC Score**: è¡¡é‡æ¨¡å‹åŒºåˆ†èƒ½åŠ›ï¼ˆ0-1ï¼Œè¶Šé«˜è¶Šå¥½ï¼‰
- **Precision**: ç²¾ç¡®ç‡ï¼ˆé¢„æµ‹ä¸ºæ­£ä¾‹ä¸­çœŸæ­£ä¸ºæ­£ä¾‹çš„æ¯”ä¾‹ï¼‰
- **Recall**: å¬å›ç‡ï¼ˆçœŸæ­£ä¾‹ä¸­è¢«æ­£ç¡®é¢„æµ‹çš„æ¯”ä¾‹ï¼‰
- **F1-Score**: ç²¾ç¡®ç‡å’Œå¬å›ç‡çš„è°ƒå’Œå¹³å‡

### æ¨¡å‹éƒ¨ç½²

è®­ç»ƒå®Œæˆåï¼Œæ¨¡å‹æ–‡ä»¶åŒ…æ‹¬ï¼š
- `model_*.pkl`: è®­ç»ƒå¥½çš„æ¨¡å‹
- `scaler_*.pkl`: ç‰¹å¾æ ‡å‡†åŒ–å™¨
- `encoders_*.pkl`: åˆ†ç±»ç‰¹å¾ç¼–ç å™¨
- `model_info_*.json`: æ¨¡å‹å…ƒä¿¡æ¯

è¿™äº›æ–‡ä»¶å¯ä»¥ç”¨äºç”Ÿäº§ç¯å¢ƒçš„é¢„æµ‹æœåŠ¡ã€‚

---

## ğŸ“š æ€»ç»“

1. **ç‰¹å¾æ–‡ä»¶æ ¼å¼**: Parquetæ ¼å¼ï¼ŒåŒ…å«35+ä¸ªç‰¹å¾
2. **è®­ç»ƒæµç¨‹**: 8æ­¥å®Œæ•´æµç¨‹ï¼Œä»æ•°æ®åŠ è½½åˆ°æ¨¡å‹ä¿å­˜
3. **æ¨¡å‹é€‰æ‹©**: æ¨èä½¿ç”¨Gradient Boostingï¼Œæ€§èƒ½è¾ƒå¥½
4. **è¯„ä¼°æŒ‡æ ‡**: AUCã€Precisionã€Recallã€F1-Score
5. **æ¨¡å‹ä¿å­˜**: åŒ…å«æ¨¡å‹ã€scalerã€encoderså’Œå…ƒä¿¡æ¯

æ›´å¤šè¯¦ç»†ä¿¡æ¯è¯·å‚è€ƒè®­ç»ƒè„šæœ¬å’Œä»£ç ç¤ºä¾‹ã€‚

