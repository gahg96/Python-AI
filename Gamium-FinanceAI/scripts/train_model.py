#!/usr/bin/env python3
"""
æ¨¡å‹è®­ç»ƒè„šæœ¬

ä½¿ç”¨æ–¹æ³•:
    python3 train_model.py --features data/extracted/customer_features.parquet --output models/
"""

import argparse
import pandas as pd
import numpy as np
import pickle
import json
from pathlib import Path
from datetime import datetime

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.metrics import (
    classification_report, 
    roc_auc_score, 
    confusion_matrix,
    precision_recall_curve
)

class ModelTrainer:
    """æ¨¡å‹è®­ç»ƒå™¨"""
    
    def __init__(self, features_path: str, output_dir: str):
        self.features_path = Path(features_path)
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        self.scaler = StandardScaler()
        self.label_encoders = {}
        self.model = None
        self.feature_cols = None
        
    def load_data(self):
        """åŠ è½½ç‰¹å¾æ•°æ®"""
        print(f"ğŸ“Š åŠ è½½ç‰¹å¾æ–‡ä»¶: {self.features_path}")
        self.features = pd.read_parquet(self.features_path)
        print(f"   âœ… åŠ è½½å®Œæˆ: {len(self.features):,} æ¡è®°å½•, {len(self.features.columns)} ä¸ªç‰¹å¾")
        return self.features
    
    def preprocess_data(self):
        """æ•°æ®é¢„å¤„ç†"""
        print("\nğŸ”§ æ•°æ®é¢„å¤„ç†...")
        
        # å¤„ç†ç¼ºå¤±å€¼
        numeric_cols = self.features.select_dtypes(include=[np.number]).columns
        self.features[numeric_cols] = self.features[numeric_cols].fillna(
            self.features[numeric_cols].median()
        )
        
        # å¤„ç†åˆ†ç±»ç‰¹å¾
        categorical_cols = ['gender', 'education', 'industry', 'city_tier', 'customer_type']
        for col in categorical_cols:
            if col in self.features.columns:
                le = LabelEncoder()
                self.features[col] = le.fit_transform(self.features[col].astype(str))
                self.label_encoders[col] = le
        
        print("   âœ… é¢„å¤„ç†å®Œæˆ")
    
    def prepare_features(self):
        """å‡†å¤‡ç‰¹å¾å’Œç›®æ ‡å˜é‡"""
        print("\nğŸ¯ å‡†å¤‡ç‰¹å¾å’Œç›®æ ‡å˜é‡...")
        
        # é€‰æ‹©ç‰¹å¾åˆ—
        self.feature_cols = [
            'age', 'gender', 'education', 'industry', 'city_tier',
            'monthly_income', 'total_assets', 'total_liabilities',
            'debt_ratio', 'debt_to_income', 'total_deposit_balance',
            'savings_rate', 'income_volatility',
            'total_loans', 'default_count', 'max_overdue_days',
            'months_as_customer', 'months_since_last_loan'
        ]
        
        # åªé€‰æ‹©å­˜åœ¨çš„åˆ—
        self.feature_cols = [col for col in self.feature_cols if col in self.features.columns]
        
        X = self.features[self.feature_cols]
        y = self.features['defaulted']
        
        print(f"   ç‰¹å¾æ•°: {len(self.feature_cols)}")
        print(f"   ç›®æ ‡å˜é‡åˆ†å¸ƒ:\n{y.value_counts()}")
        
        return X, y
    
    def split_data(self, X, y):
        """åˆ†å‰²æ•°æ®é›†"""
        print("\nğŸ“Š åˆ†å‰²æ•°æ®é›†...")
        
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=y
        )
        
        X_train, X_val, y_train, y_val = train_test_split(
            X_train, y_train, test_size=0.2, random_state=42, stratify=y_train
        )
        
        print(f"   è®­ç»ƒé›†: {X_train.shape[0]:,} æ¡")
        print(f"   éªŒè¯é›†: {X_val.shape[0]:,} æ¡")
        print(f"   æµ‹è¯•é›†: {X_test.shape[0]:,} æ¡")
        
        return X_train, X_val, X_test, y_train, y_val, y_test
    
    def scale_features(self, X_train, X_val, X_test):
        """ç‰¹å¾æ ‡å‡†åŒ–"""
        print("\nğŸ“ ç‰¹å¾æ ‡å‡†åŒ–...")
        
        X_train_scaled = self.scaler.fit_transform(X_train)
        X_val_scaled = self.scaler.transform(X_val)
        X_test_scaled = self.scaler.transform(X_test)
        
        print("   âœ… æ ‡å‡†åŒ–å®Œæˆ")
        
        return X_train_scaled, X_val_scaled, X_test_scaled
    
    def train_model(self, X_train, y_train, X_val, y_val):
        """è®­ç»ƒæ¨¡å‹"""
        print("\nğŸš€ è®­ç»ƒæ¨¡å‹...")
        
        self.model = GradientBoostingClassifier(
            n_estimators=100,
            learning_rate=0.1,
            max_depth=5,
            random_state=42,
            verbose=1
        )
        
        self.model.fit(X_train, y_train)
        
        # éªŒè¯é›†è¯„ä¼°
        val_pred_proba = self.model.predict_proba(X_val)[:, 1]
        val_auc = roc_auc_score(y_val, val_pred_proba)
        
        print(f"   âœ… è®­ç»ƒå®Œæˆ")
        print(f"   éªŒè¯é›† AUC: {val_auc:.4f}")
        
        return self.model
    
    def evaluate_model(self, X_test, y_test):
        """è¯„ä¼°æ¨¡å‹"""
        print("\nğŸ“ˆ æ¨¡å‹è¯„ä¼°...")
        
        y_pred = self.model.predict(X_test)
        y_pred_proba = self.model.predict_proba(X_test)[:, 1]
        
        # è®¡ç®—æŒ‡æ ‡
        auc = roc_auc_score(y_test, y_pred_proba)
        report = classification_report(y_test, y_pred)
        cm = confusion_matrix(y_test, y_pred)
        
        print(f"AUC Score: {auc:.4f}")
        print(f"\nåˆ†ç±»æŠ¥å‘Š:\n{report}")
        print(f"\næ··æ·†çŸ©é˜µ:\n{cm}")
        
        return {
            'auc': float(auc),
            'report': report,
            'confusion_matrix': cm.tolist()
        }
    
    def save_model(self, metrics):
        """ä¿å­˜æ¨¡å‹"""
        print("\nğŸ’¾ ä¿å­˜æ¨¡å‹...")
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # ä¿å­˜æ¨¡å‹
        model_path = self.output_dir / f'model_{timestamp}.pkl'
        with open(model_path, 'wb') as f:
            pickle.dump(self.model, f)
        
        # ä¿å­˜scaler
        scaler_path = self.output_dir / f'scaler_{timestamp}.pkl'
        with open(scaler_path, 'wb') as f:
            pickle.dump(self.scaler, f)
        
        # ä¿å­˜label encoders
        encoders_path = self.output_dir / f'encoders_{timestamp}.pkl'
        with open(encoders_path, 'wb') as f:
            pickle.dump(self.label_encoders, f)
        
        # ä¿å­˜ç‰¹å¾åˆ—è¡¨
        feature_info = {
            'feature_cols': self.feature_cols,
            'metrics': metrics,
            'timestamp': timestamp,
            'model_path': str(model_path),
            'scaler_path': str(scaler_path),
            'encoders_path': str(encoders_path)
        }
        
        info_path = self.output_dir / f'model_info_{timestamp}.json'
        with open(info_path, 'w') as f:
            json.dump(feature_info, f, indent=2, ensure_ascii=False)
        
        print(f"   âœ… æ¨¡å‹å·²ä¿å­˜:")
        print(f"     æ¨¡å‹: {model_path}")
        print(f"     Scaler: {scaler_path}")
        print(f"     Encoders: {encoders_path}")
        print(f"     ä¿¡æ¯: {info_path}")
        
        return model_path
    
    def run(self):
        """è¿è¡Œå®Œæ•´è®­ç»ƒæµç¨‹"""
        print("=" * 60)
        print("ğŸ¯ æ¨¡å‹è®­ç»ƒæµç¨‹")
        print("=" * 60)
        
        # 1. åŠ è½½æ•°æ®
        self.load_data()
        
        # 2. é¢„å¤„ç†
        self.preprocess_data()
        
        # 3. å‡†å¤‡ç‰¹å¾
        X, y = self.prepare_features()
        
        # 4. åˆ†å‰²æ•°æ®
        X_train, X_val, X_test, y_train, y_val, y_test = self.split_data(X, y)
        
        # 5. æ ‡å‡†åŒ–
        X_train_scaled, X_val_scaled, X_test_scaled = self.scale_features(
            X_train, X_val, X_test
        )
        
        # 6. è®­ç»ƒæ¨¡å‹
        self.train_model(X_train_scaled, y_train, X_val_scaled, y_val)
        
        # 7. è¯„ä¼°æ¨¡å‹
        metrics = self.evaluate_model(X_test_scaled, y_test)
        
        # 8. ä¿å­˜æ¨¡å‹
        model_path = self.save_model(metrics)
        
        print("\n" + "=" * 60)
        print("âœ… è®­ç»ƒå®Œæˆï¼")
        print("=" * 60)
        
        return model_path

if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='æ¨¡å‹è®­ç»ƒè„šæœ¬')
    parser.add_argument('--features', type=str, required=True,
                       help='ç‰¹å¾æ–‡ä»¶è·¯å¾„ (Parquetæ ¼å¼)')
    parser.add_argument('--output', type=str, default='models',
                       help='æ¨¡å‹è¾“å‡ºç›®å½•')
    
    args = parser.parse_args()
    
    trainer = ModelTrainer(args.features, args.output)
    trainer.run()


