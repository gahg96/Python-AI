#!/usr/bin/env python3
"""
æ¨¡å‹è¯„ä¼°è„šæœ¬

è¯„ä¼°æ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šçš„è¡¨ç°ï¼Œå¹¶ç”Ÿæˆè¯¦ç»†çš„è¯„ä¼°æŠ¥å‘Š

ä½¿ç”¨æ–¹æ³•:
    python3 evaluate_model.py --model models/model_20241209_143022.pkl --test data/test_features.parquet
"""

import argparse
import pickle
import json
import pandas as pd
import numpy as np
from pathlib import Path
from datetime import datetime
from typing import Dict, Any

from sklearn.metrics import (
    classification_report,
    roc_auc_score,
    roc_curve,
    precision_recall_curve,
    confusion_matrix,
    accuracy_score,
    precision_score,
    recall_score,
    f1_score
)
import matplotlib.pyplot as plt
import seaborn as sns

class ModelEvaluator:
    """æ¨¡å‹è¯„ä¼°å™¨"""
    
    def __init__(self, model_path: str, scaler_path: str = None, encoders_path: str = None):
        self.model_path = Path(model_path)
        self.model_dir = self.model_path.parent
        
        # åŠ è½½æ¨¡å‹
        print(f"ğŸ“¦ åŠ è½½æ¨¡å‹: {self.model_path}")
        with open(self.model_path, 'rb') as f:
            self.model = pickle.load(f)
        
        # åŠ è½½scaler
        if scaler_path:
            scaler_file = Path(scaler_path)
        else:
            timestamp = self.model_path.stem.split('_')[-1]
            scaler_file = self.model_dir / f'scaler_{timestamp}.pkl'
        
        if scaler_file.exists():
            with open(scaler_file, 'rb') as f:
                self.scaler = pickle.load(f)
        else:
            self.scaler = None
        
        # åŠ è½½encoders
        if encoders_path:
            encoders_file = Path(encoders_path)
        else:
            timestamp = self.model_path.stem.split('_')[-1]
            encoders_file = self.model_dir / f'encoders_{timestamp}.pkl'
        
        if encoders_file.exists():
            with open(encoders_file, 'rb') as f:
                self.encoders = pickle.load(f)
        else:
            self.encoders = {}
        
        # åŠ è½½æ¨¡å‹ä¿¡æ¯
        info_file = self.model_dir / f"model_info_{self.model_path.stem.split('_')[-1]}.json"
        if info_file.exists():
            with open(info_file, 'r') as f:
                self.model_info = json.load(f)
                self.feature_cols = self.model_info.get('feature_cols', [])
        else:
            self.feature_cols = []
        
        print("âœ… æ¨¡å‹åŠ è½½å®Œæˆ\n")
    
    def load_test_data(self, test_path: str) -> tuple:
        """åŠ è½½æµ‹è¯•æ•°æ®"""
        print(f"ğŸ“Š åŠ è½½æµ‹è¯•æ•°æ®: {test_path}")
        test_data = pd.read_parquet(test_path)
        
        # é¢„å¤„ç†ï¼ˆä¸è®­ç»ƒæ—¶ä¸€è‡´ï¼‰
        numeric_cols = test_data.select_dtypes(include=[np.number]).columns
        test_data[numeric_cols] = test_data[numeric_cols].fillna(
            test_data[numeric_cols].median()
        )
        
        # ç¼–ç åˆ†ç±»ç‰¹å¾
        for col in ['gender', 'education', 'industry', 'city_tier', 'customer_type']:
            if col in test_data.columns and col in self.encoders:
                try:
                    test_data[col] = self.encoders[col].transform(test_data[col].astype(str))
                except ValueError:
                    # å¤„ç†æœªè§è¿‡çš„å€¼
                    test_data[col] = 0
        
        # å‡†å¤‡ç‰¹å¾å’Œç›®æ ‡
        X = test_data[self.feature_cols]
        y = test_data['defaulted']
        
        print(f"   âœ… åŠ è½½å®Œæˆ: {len(test_data):,} æ¡è®°å½•")
        print(f"   ç‰¹å¾æ•°: {len(self.feature_cols)}")
        print(f"   è¿çº¦ç‡: {y.mean():.2%}")
        
        return X, y
    
    def evaluate(self, X_test, y_test) -> Dict[str, Any]:
        """è¯„ä¼°æ¨¡å‹"""
        print("\nğŸ¯ è¿›è¡Œé¢„æµ‹...")
        
        # æ ‡å‡†åŒ–
        if self.scaler:
            X_test_scaled = self.scaler.transform(X_test)
        else:
            X_test_scaled = X_test.values
        
        # é¢„æµ‹
        y_pred = self.model.predict(X_test_scaled)
        y_pred_proba = self.model.predict_proba(X_test_scaled)[:, 1]
        
        print("   âœ… é¢„æµ‹å®Œæˆ\n")
        
        # è®¡ç®—æŒ‡æ ‡
        print("ğŸ“ˆ è®¡ç®—è¯„ä¼°æŒ‡æ ‡...")
        
        metrics = {
            'accuracy': float(accuracy_score(y_test, y_pred)),
            'precision': float(precision_score(y_test, y_pred, zero_division=0)),
            'recall': float(recall_score(y_test, y_pred, zero_division=0)),
            'f1_score': float(f1_score(y_test, y_pred, zero_division=0)),
            'roc_auc': float(roc_auc_score(y_test, y_pred_proba)),
        }
        
        # åˆ†ç±»æŠ¥å‘Š
        report = classification_report(y_test, y_pred, output_dict=True)
        
        # æ··æ·†çŸ©é˜µ
        cm = confusion_matrix(y_test, y_pred)
        
        # ROCæ›²çº¿æ•°æ®
        fpr, tpr, roc_thresholds = roc_curve(y_test, y_pred_proba)
        
        # PRæ›²çº¿æ•°æ®
        precision, recall, pr_thresholds = precision_recall_curve(y_test, y_pred_proba)
        
        # è®¡ç®—æœ€ä½³é˜ˆå€¼
        f1_scores = 2 * (precision * recall) / (precision + recall + 1e-10)
        best_threshold_idx = np.argmax(f1_scores)
        best_threshold = pr_thresholds[best_threshold_idx]
        
        results = {
            'metrics': metrics,
            'classification_report': report,
            'confusion_matrix': cm.tolist(),
            'best_threshold': float(best_threshold),
            'roc_curve': {
                'fpr': fpr.tolist(),
                'tpr': tpr.tolist(),
                'thresholds': roc_thresholds.tolist()
            },
            'pr_curve': {
                'precision': precision.tolist(),
                'recall': recall.tolist(),
                'thresholds': pr_thresholds.tolist()
            },
            'test_size': len(y_test),
            'positive_samples': int(y_test.sum()),
            'negative_samples': int((y_test == 0).sum()),
            'positive_rate': float(y_test.mean())
        }
        
        return results
    
    def print_report(self, results: Dict[str, Any]):
        """æ‰“å°è¯„ä¼°æŠ¥å‘Š"""
        print("\n" + "=" * 60)
        print("ğŸ“Š æ¨¡å‹è¯„ä¼°æŠ¥å‘Š")
        print("=" * 60)
        
        metrics = results['metrics']
        print(f"\næ ¸å¿ƒæŒ‡æ ‡:")
        print(f"  AUC Score:     {metrics['roc_auc']:.4f}")
        print(f"  å‡†ç¡®ç‡:        {metrics['accuracy']:.4f}")
        print(f"  ç²¾ç¡®ç‡:        {metrics['precision']:.4f}")
        print(f"  å¬å›ç‡:        {metrics['recall']:.4f}")
        print(f"  F1åˆ†æ•°:        {metrics['f1_score']:.4f}")
        
        print(f"\næµ‹è¯•é›†ä¿¡æ¯:")
        print(f"  æ€»æ ·æœ¬æ•°:      {results['test_size']:,}")
        print(f"  æ­£æ ·æœ¬æ•°:      {results['positive_samples']:,}")
        print(f"  è´Ÿæ ·æœ¬æ•°:      {results['negative_samples']:,}")
        print(f"  æ­£æ ·æœ¬ç‡:      {results['positive_rate']:.2%}")
        
        print(f"\næ··æ·†çŸ©é˜µ:")
        cm = np.array(results['confusion_matrix'])
        print(f"              é¢„æµ‹")
        print(f"           æ­£å¸¸  è¿çº¦")
        print(f"å®é™… æ­£å¸¸  {cm[0,0]:6d} {cm[0,1]:6d}")
        print(f"     è¿çº¦  {cm[1,0]:6d} {cm[1,1]:6d}")
        
        print(f"\nåˆ†ç±»æŠ¥å‘Š:")
        report = results['classification_report']
        print(f"  ç±»åˆ« 0 (æ­£å¸¸):")
        print(f"    ç²¾ç¡®ç‡: {report['0']['precision']:.4f}")
        print(f"    å¬å›ç‡: {report['0']['recall']:.4f}")
        print(f"    F1åˆ†æ•°: {report['0']['f1-score']:.4f}")
        print(f"  ç±»åˆ« 1 (è¿çº¦):")
        print(f"    ç²¾ç¡®ç‡: {report['1']['precision']:.4f}")
        print(f"    å¬å›ç‡: {report['1']['recall']:.4f}")
        print(f"    F1åˆ†æ•°: {report['1']['f1-score']:.4f}")
        
        print(f"\næœ€ä½³é˜ˆå€¼: {results['best_threshold']:.4f}")
        print("=" * 60)
    
    def save_report(self, results: Dict[str, Any], output_path: str):
        """ä¿å­˜è¯„ä¼°æŠ¥å‘Š"""
        output_file = Path(output_path)
        output_file.parent.mkdir(parents=True, exist_ok=True)
        
        with open(output_file, 'w') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        
        print(f"\nâœ… è¯„ä¼°æŠ¥å‘Šå·²ä¿å­˜: {output_file}")
    
    def plot_curves(self, results: Dict[str, Any], output_dir: str):
        """ç»˜åˆ¶è¯„ä¼°æ›²çº¿"""
        output_dir = Path(output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # ROCæ›²çº¿
        plt.figure(figsize=(10, 6))
        fpr = results['roc_curve']['fpr']
        tpr = results['roc_curve']['tpr']
        auc = results['metrics']['roc_auc']
        
        plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {auc:.4f})', linewidth=2)
        plt.plot([0, 1], [0, 1], 'k--', label='Random')
        plt.xlabel('False Positive Rate')
        plt.ylabel('True Positive Rate')
        plt.title('ROC Curve')
        plt.legend()
        plt.grid(True, alpha=0.3)
        plt.savefig(output_dir / 'roc_curve.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        # PRæ›²çº¿
        plt.figure(figsize=(10, 6))
        precision = results['pr_curve']['precision']
        recall = results['pr_curve']['recall']
        
        plt.plot(recall, precision, label='PR Curve', linewidth=2)
        plt.xlabel('Recall')
        plt.ylabel('Precision')
        plt.title('Precision-Recall Curve')
        plt.legend()
        plt.grid(True, alpha=0.3)
        plt.savefig(output_dir / 'pr_curve.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"âœ… è¯„ä¼°æ›²çº¿å·²ä¿å­˜åˆ°: {output_dir}")

def main():
    parser = argparse.ArgumentParser(description='æ¨¡å‹è¯„ä¼°è„šæœ¬')
    parser.add_argument('--model', type=str, required=True,
                       help='æ¨¡å‹æ–‡ä»¶è·¯å¾„ (.pkl)')
    parser.add_argument('--test', type=str, required=True,
                       help='æµ‹è¯•æ•°æ®æ–‡ä»¶è·¯å¾„ (.parquet)')
    parser.add_argument('--output', type=str, default='evaluation_report.json',
                       help='è¯„ä¼°æŠ¥å‘Šè¾“å‡ºè·¯å¾„')
    parser.add_argument('--plot', action='store_true',
                       help='ç”Ÿæˆè¯„ä¼°æ›²çº¿å›¾')
    parser.add_argument('--plot-dir', type=str, default='evaluation_plots',
                       help='æ›²çº¿å›¾è¾“å‡ºç›®å½•')
    
    args = parser.parse_args()
    
    # åˆ›å»ºè¯„ä¼°å™¨
    evaluator = ModelEvaluator(args.model)
    
    # åŠ è½½æµ‹è¯•æ•°æ®
    X_test, y_test = evaluator.load_test_data(args.test)
    
    # è¯„ä¼°
    results = evaluator.evaluate(X_test, y_test)
    
    # æ‰“å°æŠ¥å‘Š
    evaluator.print_report(results)
    
    # ä¿å­˜æŠ¥å‘Š
    evaluator.save_report(results, args.output)
    
    # ç»˜åˆ¶æ›²çº¿
    if args.plot:
        evaluator.plot_curves(results, args.plot_dir)
    
    return results

if __name__ == '__main__':
    main()

